FLASK_ENV=development
export FLASK_APP=wsgi.py

# local db variables
DATABASE_USERNAME=search
DATABASE_PASSWORD=search
DATABASE_NAME=search
DATABASE_HOST=localhost
DATABASE_PORT=54332

DATABASE_TEST_USERNAME=search
DATABASE_TEST_PASSWORD=search
DATABASE_TEST_NAME=search
DATABASE_TEST_HOST=localhost
DATABASE_TEST_PORT=5432

DATABASE_DOCKER_USERNAME=docker
DATABASE_DOCKER_PASSWORD=docker
DATABASE_DOCKER_NAME=docker
DATABASE_DOCKER_HOST=docker
DATABASE_DOCKER_PORT=5432

JWT_OIDC_TEST_ISSUER="http://localhost:8081/auth/realms/demo"
JWT_OIDC_TEST_WELL_KNOWN_CONFIG="http://localhost:8081/auth/realms/demo/.well-known/openid-configuration"
JWT_OIDC_TEST_ALGORITHMS="RS256"
JWT_OIDC_TEST_AUDIENCE="search-web"
JWT_OIDC_TEST_CLIENT_SECRET="1111111111"
JWT_OIDC_TEST_JWKS_CACHE_TIMEOUT="6000"

JWT_OIDC_WELL_KNOWN_CONFIG=https://localhost:8080/auth/realms/search/.well-known/openid-configuration
JWT_OIDC_AUDIENCE=account
JWT_OIDC_ISSUER=https://localhost:8080/auth/realms/search
JWT_OIDC_ALGORITHMS=RS256
JWT_OIDC_JWKS_URI=https://localhost:8080/auth/realms/search/protocol/openid-connect/certs
JWT_OIDC_CACHING_ENABLED=True
JWT_OIDC_JWKS_CACHE_TIMEOUT=3000000

SITE_URL=http://localhost:3000
KEYCLOAK_BASE_URL=https://localhost:8080
KEYCLOAK_URL_REALM=search

CORS_ORIGIN=http://192.168.0.x:8000,http://192.168.0.x:3000

VECTOR_SEARCH_API_URL=[your-vector-search-api-url]

# LLM Provider Configuration
LLM_PROVIDER=openai  # 'openai' for Azure OpenAI API or 'ollama' for local Ollama

# Azure OpenAI Configuration
AZURE_OPENAI_API_KEY=your-api-key-here
AZURE_OPENAI_ENDPOINT=your-endpoint-url-here  # e.g., https://{your-resource-name}.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=your-model-deployment-name  # e.g., gpt-4, gpt-35-turbo
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# LLM Common Settings
LLM_TEMPERATURE=0.3  # Lower temperature for more focused, factual responses
LLM_MAX_TOKENS=1000  # Balanced for RAG responses
LLM_MAX_CONTEXT_LENGTH=8192  # Adjust based on your model's capabilities

# Ollama Configuration (required if LLM_PROVIDER=ollama)
LLM_MODEL=qwen2.5:0.5b
LLM_HOST=http://localhost:11434