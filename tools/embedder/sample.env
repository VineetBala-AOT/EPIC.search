# EPIC.search Embedder - Environment Variables Configuration
# 
# Copy this file to '.env' and update the values for your environment.
# These variables configure the connection to external services and 
# adjust the behavior of the document embedding system.

# API configuration
DOCUMENT_SEARCH_URL=https://example.com/api/public/search

# S3 Storage configuration
S3_BUCKET_NAME=your-bucket-name
S3_ACCESS_KEY_ID=your-access-key
S3_SECRET_ACCESS_KEY=your-secret-key
S3_REGION=your-region
S3_ENDPOINT_URI=https://your-s3-endpoint.com

# Database configuration
VECTOR_DB_URL=postgresql://user:password@hostname:port/database
AUTO_CREATE_PGVECTOR_EXTENSION=True     # Whether to auto-create the pgvector extension
RESET_DB=False                           # WARNING: If True, will DROP and recreate all tables on startup (dev/test only!)

# Vector database table/index naming (optional, defaults shown)
EMBEDDING_DIMENSIONS=768                 # Dimensions of the embedding vectors

# Model settings - separate models for embedding and keyword extraction
# Both default to 'all-mpnet-base-v2' if not specified
EMBEDDING_MODEL_NAME=all-mpnet-base-v2
KEYWORD_MODEL_NAME=all-mpnet-base-v2

# Document chunking configuration (optional, defaults shown)
CHUNK_SIZE=1000                      # Size of text chunks in characters
CHUNK_OVERLAP=200                    # Number of characters to overlap between chunks

# Processing configuration with intelligent auto-configuration
# The embedder automatically optimizes settings based on your hardware
FILES_CONCURRENCY_SIZE=auto          # auto (recommended), auto-full, auto-conservative, or integer
KEYWORD_EXTRACTION_WORKERS=auto      # auto (recommended), auto-aggressive, auto-conservative, or integer  
CHUNK_INSERT_BATCH_SIZE=50           # Number of chunks per database batch (25 default, 50 for high-RAM systems)

# Keyword extraction performance modes - TEST DIFFERENT MODES FOR SPEED vs QUALITY
KEYWORD_EXTRACTION_MODE=fast         # standard, fast, simplified

# Performance mode options:
# KEYWORD_EXTRACTION_MODE=standard    → Full KeyBERT quality (baseline, slowest)
# KEYWORD_EXTRACTION_MODE=fast        → KeyBERT optimized + batch processing (2-4x faster, query-compatible)
# KEYWORD_EXTRACTION_MODE=simplified  → TF-IDF ultra-fast (10-50x faster, may affect search quality)

# Auto-configuration options:
# FILES_CONCURRENCY_SIZE=auto         → Half cores for 16+ CPU systems (prevents over-parallelization)
# FILES_CONCURRENCY_SIZE=auto-full    → All CPU cores (maximum parallelism)
# FILES_CONCURRENCY_SIZE=auto-conservative → Quarter CPU cores (resource-constrained)
# 
# KEYWORD_EXTRACTION_WORKERS=auto     → Optimized for KeyBERT bottleneck (2 threads for 16+ cores)
# KEYWORD_EXTRACTION_WORKERS=auto-aggressive → 4 threads per process (maximum keyword parallelism)
# KEYWORD_EXTRACTION_WORKERS=auto-conservative → 1 thread per process (minimal contention)
#
# Example results for 32-core server: auto = 16 processes × 2 threads = 32 total threads (100% CPU utilization)

# API pagination configuration (optional, defaults shown)
GET_PROJECT_PAGE=1                   # Number of projects to fetch per API call
GET_DOCS_PAGE=1000                   # Number of documents to fetch per API call

# OCR (Optical Character Recognition) configuration for scanned PDFs
OCR_ENABLED=true                     # Whether to process scanned PDFs with OCR (true/false)
OCR_PROVIDER=tesseract               # OCR provider: 'tesseract' (local) or 'azure' (cloud)

# Tesseract OCR settings (when OCR_PROVIDER=tesseract)
# TESSERACT_PATH=C:\Program Files\Tesseract-OCR\tesseract.exe  # Path to Tesseract executable (auto-detected if not set)
OCR_DPI=300                          # DPI for OCR image conversion (higher = better quality but slower)
OCR_LANGUAGE=eng                     # Language code for OCR (eng, fra, deu, etc.)

# Azure Document Intelligence OCR settings (when OCR_PROVIDER=azure)
# AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://yourresource.cognitiveservices.azure.com/
# AZURE_DOCUMENT_INTELLIGENCE_KEY=your_api_key_here