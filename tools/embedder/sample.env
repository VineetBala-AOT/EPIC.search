# EPIC.search Embedder - Environment Variables Configuration
# 
# Copy this file to '.env' and update the values for your environment.
# These variables configure the connection to external services and 
# adjust the behavior of the document embedding system.

# API configuration
DOCUMENT_SEARCH_URL=https://example.com/api/public/search

# S3 Storage configuration
S3_BUCKET_NAME=your-bucket-name
S3_ACCESS_KEY_ID=your-access-key
S3_SECRET_ACCESS_KEY=your-secret-key
S3_REGION=your-region
S3_ENDPOINT_URI=https://your-s3-endpoint.com

# Database configuration
VECTOR_DB_URL=postgresql://user:password@hostname:port/database
AUTO_CREATE_PGVECTOR_EXTENSION=True     # Whether to auto-create the pgvector extension
RESET_DB=False                           # WARNING: If True, will DROP and recreate all tables on startup (dev/test only!)

# Main database connection pool (for setup and admin operations)
DB_POOL_SIZE=10                          # Number of persistent connections for main operations
DB_MAX_OVERFLOW=20                       # Additional connections when needed
DB_POOL_RECYCLE=900                      # Seconds before recycling connections (15 minutes)
DB_POOL_TIMEOUT=120                      # Seconds to wait for connection from pool (2 minutes)
DB_CONNECT_TIMEOUT=60                    # Seconds to establish initial connection (1 minute)

# Worker process database settings (for document processing - prevents P03 errors)
WORKER_POOL_SIZE=1                       # Single connection per worker process
WORKER_MAX_OVERFLOW=2                    # Minimal overflow per worker
WORKER_POOL_TIMEOUT=30                   # Shorter timeout for workers (30 seconds)
WORKER_CONNECT_TIMEOUT=30                # Shorter connect timeout for workers (30 seconds)

# High compute server settings (16+ CPU cores) for 8+ hour runs:
# DB_POOL_SIZE=20                        # More connections for main operations
# DB_MAX_OVERFLOW=40                     # More overflow capacity
# DB_POOL_RECYCLE=600                    # 10 minutes - shorter for stability on long runs
# DB_POOL_TIMEOUT=300                    # 5 minutes for patient connection waiting
# DB_CONNECT_TIMEOUT=120                 # 2 minutes for network delays

# Vector database table/index naming (optional, defaults shown)
EMBEDDING_DIMENSIONS=768                 # Dimensions of the embedding vectors

# Model settings - separate models for embedding and keyword extraction
# Both default to 'all-mpnet-base-v2' if not specified
EMBEDDING_MODEL_NAME=all-mpnet-base-v2
KEYWORD_MODEL_NAME=all-mpnet-base-v2

# Document chunking configuration (optional, defaults shown)
CHUNK_SIZE=1000                      # Size of text chunks in characters
CHUNK_OVERLAP=200                    # Number of characters to overlap between chunks

# Processing configuration
FILES_CONCURRENCY_SIZE=8            # Number of concurrent document processing workers
KEYWORD_EXTRACTION_WORKERS=1         # Number of keyword extraction threads per worker
CHUNK_INSERT_BATCH_SIZE=25           # Number of chunks per database batch

# Processing configuration (16+ CPU cores)
FILES_CONCURRENCY_SIZE=16            # Number of concurrent document processing workers
KEYWORD_EXTRACTION_WORKERS=2         # Number of keyword extraction threads per worker
CHUNK_INSERT_BATCH_SIZE=50           # Number of chunks per database batch

# Keyword extraction performance modes - TEST DIFFERENT MODES FOR SPEED vs QUALITY
KEYWORD_EXTRACTION_MODE=fast         # standard, fast, simplified

# Performance mode options:
# KEYWORD_EXTRACTION_MODE=standard    → Full KeyBERT quality (baseline, slowest)
# KEYWORD_EXTRACTION_MODE=fast        → KeyBERT optimized + batch processing (2-4x faster, query-compatible)
# KEYWORD_EXTRACTION_MODE=simplified  → TF-IDF ultra-fast (10-50x faster, may affect search quality)

# API pagination configuration (optional, defaults shown)
GET_PROJECT_PAGE=1                   # Number of projects to fetch per API call
GET_DOCS_PAGE=1000                   # Number of documents to fetch per API call

# OCR (Optical Character Recognition) configuration for scanned PDFs
OCR_ENABLED=true                     # Whether to process scanned PDFs with OCR (true/false)
OCR_PROVIDER=tesseract               # OCR provider: 'tesseract' (local) or 'azure' (cloud)

# Tesseract OCR settings (when OCR_PROVIDER=tesseract)
# TESSERACT_PATH=C:\Program Files\Tesseract-OCR\tesseract.exe  # Path to Tesseract executable (auto-detected if not set)
OCR_DPI=300                          # DPI for OCR image conversion (higher = better quality but slower)
OCR_LANGUAGE=eng                     # Language code for OCR (eng, fra, deu, etc.)

# Azure Document Intelligence OCR settings (when OCR_PROVIDER=azure)
# AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=https://yourresource.cognitiveservices.azure.com/
# AZURE_DOCUMENT_INTELLIGENCE_KEY=your_api_key_here

# Word Document Processing (DOCX/DOC files)
WORD_PROCESSING_ENABLED=true         # Enable/disable Word document processing
WORD_CHUNK_SIZE=2000                 # Size of Word document chunks (characters per "page")
WORD_PRESERVE_FORMATTING=false       # Whether to preserve Word document formatting in text extraction